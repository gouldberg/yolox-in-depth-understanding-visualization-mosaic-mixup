# yolox-in-depth-understanding-visualization-mosaic-mixup
- I have had some opportunities to work with **YOLOX object detection** (https://github.com/Megvii-BaseDetection/YOLOX). In order to improve detection performance in limited situation, meaning that only YOLOX is available and no other drastic approach change or even small model change is not allowed, I have reversely engineered YOLOX original codes in depth step-by-step in order to **get better understanding for parameter tuning**.
  - **The scripts for step-by-step understanding** is in folder 'yolox_detail_in-depth step-by-step'
- **Image augmentation at training by mosaic and mixup**** is critical for YOLOX training and it is the key for parameter tuning. I have tried to **visualize the process of mosaic and mixup** to understand what characteristics of images are really input to YOLOX deep learning network during training.
  - **The sample real images generated by mosaic + mixup** are in folder 'sample_mosaicmixup', which is really interesting, and a bit surprising some time.
- **At inference**, YOLOX learned model put priority in some area in object based on extracted (different size) features and inference algorithm (which is also required to be understood). Those prioritized areas are also interesting. If some part of objects are occluded by other objects, which part are put priority by learned model ?  If features to differentiate from others are very limited, the learned model is sensitive to the scarce features ?
  - **The sample images with prioritized area point at inference** are in folder 'sample_inference'
- **The scripts for those visualization** described above are in folder 'yolox_visualization'
